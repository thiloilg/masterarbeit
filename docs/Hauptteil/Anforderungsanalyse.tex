\chapter{Anforderungsanalyse}
\label{chap:anforderungsanalyse}

In der Anforderungsanalyse geht es um das Erörtern der Anforderungen an die Software.
Als erstes gilt es in \Cref{sec:weristmeinezielgruppe} die Zielgruppe der Anwendung zu
definieren und zu analysieren. Die daraufhin folgende Anforderungsanalyse wird in zwei
Grundrubriken unterteilt. Funktionale sowie nicht-funktionale Anforderungen. Die
nicht-funktionalen Anforderungen sind in die Unterabschnitte Gestaltung, Flexibilität
und Codebasis gegliedert.

GBO Datacomp hat seinerseits Anforderungen an die Software in Form eines Informationsblattes
bereitgestellt. Diese Anforderungen werden in den Folgeabschnitten miteinbezogen.

\section{Wer ist die Zielgruppe?}
\label{sec:weristmeinezielgruppe}
Die in der Masterarbeit zu entwickelnde Softwarelösungen
soll den Bestandkunden von GBO Datacomp eine moderne Alternative geben, Daten 
auf mobilen Endgeräten wie Tablets, Smartphones aber auch Desktop PCs auswerten zu können.
Desweiteren muss die Software Neukunden ansprechen und zum Kauf überzeugen. GBO Datacomp
hat Kunden in vielen verschiedenen Branchen. So hat das Unternehmen Kunden in der
Automobilindustrie, in der Metall- und Gussindustrie, in der Lebensmittelindustrie,
in der Steinverarbeitung, in der Verpackungsindustrie, in dem Druckgewerbe, in der
Zahntechnik, der Kundstoffverarbeitung, Holzindustrie, Möbelindustrie, Metall- und Gussindustrie sowie
der Steinverarbeitung.\cite{GBODatacompBranchenloesungen}

Eines der wichtigsten Produkte von GBO Datacomp ist BisoftMES, eine Software zur Erfassung von
Maschinen- und Betriebsdaten. Diese Daten können Auftragsdaten, Schichtdaten und Maschinendaten sein.
So können die Benutzer der Software Trendanalysen durchführen, Schicht- und Maschinendaten vergleichen und
Leistungskennzahlen auswerten, um so möglichen Störungen und Ausfällen im Betrieb vorzubeugen.

Wer genau wird das agile Dashboard zur Betriebs- und Maschinendatenauswertung
benutzen? Dies kann sich je nach Betrieb unterscheiden. So interagieren von der
Managementebene aus über den Produktionsleiter bis hin zum Schichtarbeiter Benutzer mit dem
Dashboard. Eines der Dashboards könnte an einem speziell für die Softwarelösung
installierten Bildschirm in einer Produktionshalle angezeigt werden. Anderenfalls könnte aber
auch der Produktionsleiter in der Mittagspause mit seinem Handy die aktuellen Leistungskennzahlen 
(KPIs) abrufen. Viele Produktionshallen besitzen alte Windows-Rechner, die nicht
mit den neusten Browsern ausgestattet sind. Andere Betriebe besitzen Endgeräte mit Touchscreens.
Fassen wir all diese möglichen Szenarien zusammen, wird uns eines sehr deutlich. Von der
Softwarelösung wird eine gewisse Flexibilität abverlangt, die nicht zu unterschätzen ist.

\section{Funktionale Anforderungen}
\label{sec:funktionaleanforderungen}
In diesem Abschnitt werden die Anforderungen an die Funktionalität der Software analysiert.
Die erste und bedeutsamste Funktionalität ist
die möglichst agile Erstellung eines Dashboards. Die Aufgabe des Dashboards ist es, Daten
zu visualisieren. Um dies zu ermöglichen, benötigt das Dashboard Daten. Diese Daten
müssen aus einer Datenquelle geladen werden. GBO Datacomp stellt hierfür eine REST API
zur Verfügung. Für eine Datenquelle ist somit gesorgt. Um die Daten innerhalb der Anwendung
einem Dashboard zur Verfügung zu stellen, müssen die Daten zu einem passenden Format verarbeitet werden.
Zu guter Letzt muss noch definiert werden, welche Daten in welchem Dashboard verwendet
werden. Diese vier Funktionalitäten sind der Kern der Anwendung. In diesem Abschnitt
werden zuerst die Anforderungen der Kernfunktionalitäten genauer analysiert. Daraufhin werden die Anforderungen
für die Authentifizierung der Benutzer, die Rechteverwaltung, die Auswahl der Sprache,
die Navigation durch die Software, die Suche nach Ressourcen in der Anwendungen,
die Filterung der darzustellenden Daten im Dashboard sowie für die Möglichkeit,
die Anwendung auch offline zu verwenden, erörtert.

Was sind die Anforderungen für die Erstellung des agilen Dashboards? Die Anwendung und
so auch die Erstellung des Dashboards sollte selbsterklärend sein. Daraus resultiert, 
dass die Anwendung möglichst einfach aufgebaut werden sollte. Einstellungsmöglichkeiten,
die Nebenrollen in der Gesamtfunktionalität der Software darstellen, sollten von der
Software selbst entschieden werden. Ein Beispiel hierfür ist die Justierung der
einzelnen Komponenten eines Dashboards. Der Prozess der Erstellung eines Dashboards
sollte wie folgt aussehen. Ein Benutzer kann in einem intuitiven Verfahren,
einzelne Komponenten auswählen und diese an bestimmte Positionen im Bildschirm
ziehen. Desweiteren kann er die Komponenten
entfernen und ersetzen. Wirft man einen Blick auf andere BI Softwarelösungen,
stellt man fest, dass die Anzahl der Komponenten der Anwendung im Lauf der Zeit
stetig anwachsen kann. Somit ist die effiziente Auswahl, auch aus einer großen
Anzahl an Komponenten, pflicht.

Was sind die Anforderungen für das Laden der Daten in die Anwendung? Neben der Flexibilität,
die in \Cref{subsec:flexibilitaet} erörtert wird, muss das Laden von Daten schnell und zuverlässig
sein. Typische Vorgehensweisen der Anwender sind das Auswerten von Maschinendaten. Dabei kann es
sich um die Auslastung sowie Stör- und Ausfälle der Maschinen handeln. Es ist bedeutsam,
das der Anwender rechtzeitig von diesen Ereignissen informiert wird. Unsere
Aufmerksamkeit gilt also auch der zeitnahen Aktualisierung der Daten in den Dashboards. Nur
so kann sichergestellt werden, dass der Anwender auch zeitnah auf in den Daten erkenntliche
Ereignisse reagieren kann. In Anlehnung dieser Feststellung liegt es nahe, den Benutzer
über bestimmte Ereignisse per Benachrichtigung zu informieren. So kann man
eine Benachrichtigung auf das Smartphone des Benutzers schicken, sobald ein zuvor definierter
Schwellwert überschritten wird.

Welche Anforderungen sind für die Verarbeitung der Daten notwendig? Daten können Fehler oder Lücken
beinhalten. So kann in einer Spalte, die das ISO Datumsformat beinhalten, fälschlicherweise ein
ungültiger Wert vorkommen oder dieser komplett fehlen. Zugegebenermaßen reicht es für einen MVP, ein
minimal überlebensfähiges Produkt, aus, diese Unreinheiten so zu behandeln, dass die Anwendung
eine informationsreiche Fehlerwarnung an den Benutzer überliefert oder alternativ einen Standartwert
für fehlende Felder definiert.

Wie werden die Dashboards mit den nötigen Daten versorgt? Grundsätzlich ist klar davon auszugehen,
dass die Anwendung einen Zuweisungsmechanismus bereitstellt. Um von einer komplexeren, eingehenden
Datenstruktur zu einer für die Visualisierung geeigneten Datenstruktur zu kommen, müssen die gewollten
Daten ausgewählt und zugewiesen werden. Dem Benutzer selbst ist ein, ähnlich wie bei der Erstellung
des Dashboards, agiler Prozess zur Zuweisung der Daten ermöglicht. Ein Benutzer will auf einem
Dashboard unterschiedliche Quellen von Daten vergleichen. Somit muss die Möglichkeit bestehen,
unterschiedliche Zuweisungen für die Diagramme eines Dashboards zu tätigen.

Anschließend werden die Anforderungen für die Benutzerauthentifizierung und die Rechteverwaltung erörtert.
Sicherheit muss in jeder Anwendung, die mit Kundendaten interagiert, von großer Bedeutung sein.
Somit ist es naheliegend, dass für die heutigen IT-Sicherheitsstandards gesorgt wird. Progressive
Web Apps sind per Definition außerhalb der Entwicklung nur über HTTPS zu übertragen.\cite[S. 16]{KevinFrankPWAMasterarbeit}
OWASP (Open Web Application Security Project) veröffentlichte 2017 einen Bericht über die zehn
bedeutsamsten Sicherheitsrisiken von Webanwendungen. Darunter sind unter anderem die Einschleusung
von Quellcode durch mangelnde Maskierung, fehlerhafte Authentifizierungsverfahren, Preisgabe von sensiblen Daten,
XSS-Attacken und Fehlkonfigurationen von Sicherheitseinstellungen.\cite[S. 4]{OWASPTopTen}
Um die Kundendaten vor Angriffen zu schützen, liegt es nahe, dass diese Sicherheitsrisiken bei der
Entwicklung der Anwendung beachtet werden. Zum Funktionsumfang gehört dazu,
dass ein Benutzer ein Konto sicher anlegen, sich anmelden, abmelden und sein Konto auch wieder
löschen kann. Die Rechteverwaltung konzentriert sich laut GBO Datacomp vorerst auf die elementaren
Grundeigenschaften.

Da GBO Datacomp ihre Softwarelösungen auch an nicht deutschsprachige Kunden verkauft, ist es notwendig,
die Anwendung multilingual zu gestalten. Die Funktionalität, die Sprache der Anwendung zu verändern,
ist somit Teil der Anforderungen. Damit der Benutzer nicht bei jedem Besuch die Sprache neu einstellen
muss, gilt es die Spracheinstellungen im Browser zu persistieren. GBO Datacomp merkt hier an, dass diese
unabhängig von denen des Browsers gespeichert werden müssen.

Ein Benutzer will ohne großen Aufwand von einen Dashboard zu einem anderen gelangen. Er will kurz seine Profileinstellungen
einsehen, daraufhin an der Datenzuweisung arbeiten und dann die Dokumentation lesen.
Hierfür benötigt die Anwendung eine benutzerfreundliche Navigation. Ist die Anzahl der Ressourcen zu 
groß, muss die Anwendung einen automatischen Seitenumbruch einführen. Um die Suche nach speziellen
Ressourcen zu erleichtern, ist eine zentrale Suche erwünscht.

In der aktuellen BisoftMES Softwarelösung von GBO Datacomp kann man die für die Anzeige relevanten Daten filtern.\cite[S. 14]{BisoftMESHandbuch}
Es liegt nahe, dass diese Funktionalität auch bei einer webbasierten Anwendung von Benutzern erwartet wird. Die Möglichkeit, Daten
anhand von Filterkriterien einzuschränken, ist somit Teil der Anforderung an die Funktionalität der Softwarelösung.

Zu allerletzt gilt es die Anforderungen an die Anwendung im Falle einer fehlenden Netzwerkverbindung
zu erörtern. GBO Datacomp erläutert in ihrem Informationsblatt zu der Masterarbeit, dass dem Benutzer
ermöglicht werden sollte, geladene Daten offline weiterzuverwenden. Dies sollte solang möglich sein,
bis der Benutzer wieder Zugang zum Netzwerk erlangt. Bei Wiedererlangung der Netzwerkverbindung werden
die Daten automatisch aktualisiert. Das An- und Abmelden muss über die Internetverbindung
geschehen. Mozilla schreibt über die Obergrenze der Speicherkapazität der IndexedDB im Browser,
dass diese dynamisch ist. Die Obergrenze werde durch verschiedene Faktoren wie dem freigegebenen
Festplattenspeichervolumen und einer Obergrenze je Domain realisiert. Dabei kann die Domainspezifische
Obergrenze von einem Minimum von 10 Megabyte bis zu einem Maximum von aktuell 2 Gigabyte reichen.\cite{MozillaStorageLimit}
Die Verwendung der Daten ohne Netzzugriff ist bedingt möglich. Ist dies nicht möglich, gilt es
eine Offline-Rückfallseite bereitzustellen.

\section{Nicht-funktionale Anforderungen}
\label{sec:nichtfunktionaleanforderungen}
In diesem Abschnitt werden die nicht-funktionalen Anforderungen analysiert.
In \Cref{subsec:gestaltung} geht es um die graphische Benutzeroberfäche sowie
deren Benutzerfreundlichkeit. In \Cref{subsec:flexibilitaet} wird die geforderte
Flexibilität an die Softwarelösung erörtert. Zuallerletzt werden in \Cref{subsec:codebasis}
Anforderungen an die Entwicklungsumgebung und den Entwicklungsprozess gestellt.

\subsection{Gestaltung}
\label{subsec:gestaltung}
Welche Anforderungen gibt es bei der Gestaltung der Anwendung oder noch spezieller 
der grafischen Oberfläche zu beachten? Wie bereits in der Zielgruppenanalyse in \Cref{sec:weristmeinezielgruppe}
angemerkt, werden die verschiedensten Endgeräte auf die Software zugreifen.
GBO Datacomp erwähnt in ihrem Informationsblatt die Fokussierung auf folgende
drei Gerätetypen: Desktop-PCs, Tablets und Smartphones. Man kann davon ausgehen, dass die
Softwarelösung als Eingabeschnittstelle neben der Maus und der Tastatur auch mit einem Touchscreen
gesteuert werden kann. GBO Datacomp weißt speziell auf die unterschiedlichen Bildschirmgrößen
hin. So soll die Anwendung auf verschiedene Bildschirmgrößen reagieren und den Inhalt je nach
Größe zugänglich bereitstellen. Geräte des gleichen Typs sollen keine signifikanten Unterschiede
in der Darstellung aufweisen.

Man kann davon ausgehen, dass neben den unterschiedlichen Gerätetypen auch unterschiedliche
Betriebssysteme verwendet werden. Laut StatCounter verteilt sich der weltweite Marktanteil
an Betriebssystemen laut einer Analyse vom Dezember 2019 wie folgt auf. Android führt mit
40,47\%, gefolgt von Windows mit 34,2\%, iOS mit 14,92\% und macOS mit 7,24\%. Linux mit 0,83\% und weitere
Betriebssysteme mit 1,24\% bilden hierbei eine Minderheit.\cite{StatCounterOSMarketShare} Betrachtet man den
Nutzungsanteil der Browser im Oktober 2019 weltweit, führt Chrome mit 64,92\%, gefolgt von Safari mit 15,97\%,
Firefox mit 4,33\%, Samsung Internet mit 3,29\%, der primär im asiatischen Bereich genutzte Browser UC
mit 2,94\%, Opera mit 2,34\%, Edge mit 2,05\%, Internet Explorer mit 1,98\%, Android mit 0,59\% und Sonstige mit 1,59\%.\cite{StatCounterBrowserMarketShare}
Diese Kennzahlen geben einen groben Überblick über die aktuelle Lage des Marktes. Der Markt wird klar von
Google und Microsoft Produkten dominiert. Allerdings variiert der Nutzungsanteil je nach Branche.
Um ein besseres Bild über die verwendeten Technologien möglicher Kunden zu erhalten,
lohnt es sich ein Blick auf die Produkte von GBO Datacomp selbst zu werfen.
BisoftMES, eines der zentralen Produkte von GBO Datacomp, "ist eine unter Microsoft.net
entwickelte Maschinen- und Betriebsdatenerfassungssoftware".\footnote{Aus dem BisoftMES Benutzerhandbuch\cite[S. 7]{BisoftMESHandbuch}}
Es liegt nahe, dass auch die Bestandskunden von GBO Datacomp Produkte aus der Microsoft
Produktpalette verwenden. Eine Unterstützung von Browsern wie Internet Explorer und Edge
wäre daher wünschenswert.

Die Anwendung sollte als Whitelabel Produkt eingesetzt werden können. Dies bedeutet soviel wie,
dass die GUI der Anwendung mit geringem Aufwand an das Erscheinungsbild eines Unternehmens angepasst
werden kann. Beispiele dafür gibt es in der Softwareindustrie häufig. So kann man bei
der Community Edition von Gitlab das Logo sowie das Farbschema der Anwendung in den Einstellungen
anpassen.\cite{GitlabDocs}

GBO Datacomp beschreibt in den Anforderungen des zu Beginn der Arbeit überreichten
Informationsblattes konkret, dass die Benutzbarkeit der Software ohne großen Schulungsaufwand möglich sein soll.
Dies steht im klaren Kontrast zu konkurrierenden Softwarelösungen im BI Sektor.
So bietet QlikTech, ein führendes Unternehmen im Bereich Business Intelligence,
weltweit Kurse für ihr Softwareprodukt Qlik Sense an. Die Kurse haben in der
Regel eine Zeitspanne zwischen drei bis fünf Tagen.\cite{QlikSenseTraining}
Dies ist auch verständlich angesichts der Tatsache, dass QlikTech in einigen
ihrer Produkte eine eigene SQL ähnliche Skriptsprache verwenden.\cite{QlikSenseScriptLanguage}
Um eine klare, selbsterklärende Gestaltung der Anwendung zu erlangen,
muss an der Komplexität der Funktionalität gespart werden. Des weiteren
gilt es darauf zu achten, dass eine klare Abstraktion der Funktionalitäten
gegeben ist. Um den Lernaufwand zu verringern, muss bekannte Technologien
der Erfindung neuer vorgezogen werden. Falls man jedoch beweisen kann,
dass die neu erfundene Technologie das im Fokus stehende Problem effizienter löst
sowie einfacher zu erlernen ist, spricht nichts dagegen, diese Technologie der bereits
etablierten Technologie vorzuziehen.

Fasst man die oben analysierten Anforderungen in Bezug auf die grafische Benutzeroberfläche
zusammen, kommt man auf folgendes: Die Benutzeroberfläche besitzt ein Responsive Design,
das sich auf die Gegebenheiten des jeweiligen Gerätes anpasst. Unterstützung
für die führenden Browser, Edge und Internet Explorer inbegriffen, ist gegeben.
Das Design ist mit wenig Aufwand an das Erscheinungsbild eines Unternehmens anpassbar.
Das GUI verfolgt eine klare, benutzerfreundliche Richtlinie.

\subsection{Flexibilität}
\label{subsec:flexibilitaet}
Was sind die Anforderungen an die Flexibilität der Software? Die Anforderungen lassen sich in zwei
Bereiche gliedern. Im ersten Bereich geht es um die Flexibilität der Software in Bezug auf die Daten. Hierbei wird ein bedeutsames
Augenmerk auf den Umgang der Software mit der Beschaffenheit der Datenquellen und der Daten selbst gelegt. Im zweiten Bereich
wird die Anforderung in Bezug auf die Flexibilität der Visualisierung der Daten erörtert.

Die Datenquelle ist wie bereits in \Cref{sec:funktionaleanforderungen} erwähnt von GBO Datacomp als REST Schnittstelle
bereitgestellt. Über die API lassen sich Schichtdaten, Kennzahlen über die Gesamtanlageneffektivität (OEE), Arbeitsplätze
sowie Arbeitsplatzgruppen abfragen. Ein typisches Szenario ist die Abfrage von aktuellen Schichtdaten eines bestimmten
Arbeitsplatzes. Hierfür müssen zwei Aufrufe gegen die API durchgeführt werden. Zum einen muss ein Aufruf gegen die
Arbeitsplätze durchgeführt werden. Einer der Arbeitsplätze muss ausgewählt werden, um damit die aktuellen Schichtdaten
in Bezug auf diesen Arbeitsplatz zu erlangen. Um bestimmte Daten zu erhalten, benötigen wir also mehrere, aufeinanderfolgende
Aufrufe gegen die API. Diese Aufrufe sind voneinander abhängig. Diese Erkenntnis ist für den späteren Gestaltungsprozess
der Datenbeschaffung von großer Bedeutung.

In einem Gespräch mit GBO Datacomp stellte sich folgendes heraus. GBO Datacomp liefert unterschiedliche Versionen ihrer
Softwarelösungen an unterschiedliche Kunden aus. Dabei variiert auch die Datenstruktur und somit die Beschaffenheit der
Schnittstelle. Diese Beschaffenheit ist essentiell für die Softwarearchitektur dieser Arbeit. Man kann davon ausgehen,
dass GBO Datacomp ihre Softwarelösungen über die nächsten Jahre hinweg immer weiter verändern und anpassen wird.
Es ist gut möglich, dass GBO Datacomp Softwarelösungen in Zukunft komplett austauschen wird. Anzunehmen,
dass die Beschaffenheit der Schnittstelle unverändert bleibt, ist somit ein fataler Fehler. Nun gibt es zwei Optionen.
Erstens: Man entwickelt für jede Änderung der Schnittstelle eine neue Version seiner Software. Zweitens:
Die Information über die Beschaffenheit der Schnittstelle kommt in die Datenbank und nicht in den
Quellcode.

Vergleicht man die zwei im vorherigen Absatz genannten Optionen anhand des Implementierungsaufwandes,
kommt man auf folgendes Resultat: Auf kurze Sicht gewinnt die erste Option. Sie ist einfach zu implementieren.
Man programmiert die benötigten Aufrufe der Datenquellen in der angeforderten Reihenfolge in den Quellcode.
Möglicherweise fehlt noch ein bestimmter Aufruf, also fügt man diesen hinzu.
Option zwei ist um einiges schwerer zu implementieren. So muss man die Beschaffenheit sowie die Reihenfolge der Anfragen
gegen die API in der Datenbank persistieren, diese auslesen und ausführen. Es ist möglich, dass die Daten zwischen den
Anfragen verarbeitet werden müssen. Dieser Lösungsansatz fordert eine durchdachte Softwarearchitektur. Wie sieht es mit den
zwei Optionen auf lange Sicht aus? Option zwei ist mit der Implementierung fertig. Die Softwarearchitektur
benötigt für folgende Änderungen der Schnittstelle nur einen neuen Datenbankeintrag. In Kontrast dazu steht Option eins.
Hier muss für jede Änderung der Schnittstelle der Quellcode angepasst werden. Dies ist ein niemals endender Prozess.
Der Implementierungsaufwand ist also unendlich.

Eins gilt es bei dem zuvor durchgeführten Vergleich allerdings zu beachten. Beide Optionen haben ihre Daseinsberechtigung
und machen je nach Situation auch durchaus Sinn. Es wäre falsch zu sagen, dass eine dieser beiden Optionen die richtige ist. Für einen
Machbarkeitsnachweis \footnote{In der Businesswelt spricht man hier oftmals von einem POC (Proof of Concept).}
ist Option eins eine gültige Lösung. Wenn der Änderungsaufwand im Quellcode sehr gering ausfällt oder sich
die Beschaffenheit der Schnittstelle nur sehr selten ändert, ist Option eins plausibel und eventuell kostengünstiger.
Aufgrund der oben genannte Fakten ist davon auszugehen, dass die Softwarelösung mit unterschiedlichen Schnittstellen arbeiten muss.
Die Beschaffenheit der Schnittstellen kann sich stetig anpassen. Um diese Anforderung an die Flexibilität
der Software bereitzustellen, verfolgt diese Arbeit den Ansatz aus Option zwei.

Die Anforderungen an die Flexibilität der Software in Bezug auf die Kommunikation mit Schnittstellen wurde
ausführlich diskutiert. Wie sieht es aber mit der Anforderung an die Flexibilität der Daten selbst aus?
Die von GBO Datacomp bereitgestellte REST API ist mit einer Swagger Dokumentation basierend auf der
OpenAPI-Specification 3.0 ausgestattet. Die Hierarchie einer Anfragen an eine Ressourcen kann mehrere Stufen von Objekten beinhalten.
Bei der Anfrage an die OEE muss der Arbeitsplatz sowie eine Zeitspanne angegeben werden. Als Resultat bekommt man eine 
in JSON formatierte Antwort. Auf der äußeren Ebene befindet sich ein Array, welches Objekte beinhaltet, die wieder ein
Array und Objekte beinhalten. Analysiert man die Antworten aller abzufragenden Routen der bereitgestellten REST API anhand der Kardinalität,
kommt man auf folgendes Resultat: Die gefundenen Kardinalitäten beschränken sich auf \(1:1\) und \(1:N\). Dies macht auch
durchaus Sinn, da ohne Verweise auf einzigartige Identifikatoren innerhalb der JSON Datenstruktur keine weiteren
Kardinalitäten darstellbar sind.\footnote{Bei dieser Aussage werden untypische Implementationen,
die ohne weitere Informationen nicht eindeutig zu verstehen sind, ausgeschlossen.}
Daraus zu schlussfolgern, dass es keine \(N:N\)-Beziehungen geben wird, ist allerdings falsch.
So können Ressourcen unterschiedlicher Abfragen untereinander durchaus \(N:N\)-Beziehungen beinhalten. Bei der Erstellung
einer flexiblen Softwarelösung muss also eine mögliche \(N:N\)-Beziehungen unterhalb der durch die Abfragen erhaltenen
Ressourcen berücksichtigt werden.

Bei einer Anfrage an einen Endpunkt der API, kann das zurückgelieferte JSON-Objekt verschiedene Tiefen aufweisen. 
Um die Anforderung an die Flexibilität der Software in Bezug auf die Tiefe der zu verarbeitenden JSON-Objekte zu
erörtern, muss die maximale Tiefe herausgefunden werden. Bei der bereitgestellten REST API beträgt die tiefste
Hierarchie zwei Stufen. Ein ummantelndes Array was mehrere Objekte beinhaltet, welche wiederum ein Array mit Objekten
beinhalten. Es ist auch hier davon auszugehen, dass sich die maximale Tiefe der Datenstruktur verändern kann.
Die Software ist so flexibel zu gestalten, dass diese auch mit variablen Tiefen der JSON-Objekte umgehen kann.

Als nächstes werfen wir einen Blick auf die Flexibilität in Bezug auf die Visualisierung der Daten. Wie flexibel
muss die Visualisierung der Daten sein? Mit was für einer Vielfalt an Diagrammen ist zu rechnen? Wie sieht es
mit dem stetigen Hinzufügen neuer Diagramme aus? In dem von GBO Datacomp ausgehändigten Informationsblatt
werden einige Datenvisualisierungen aufgezeigt. Zu den bedeutsamsten Diagrammen gehören Balkendiagramme,
Säulendiagramme, Liniendiagramme, Kreisdiagramme, Ringdiagramme sowie additive Diagramme. Desweiteren werden
einzelne KPIs mit verschiedenen Visualisierungen dargestellt. So ändert sich die Farbe der OEE je nach
Wert von grün über gelb bis hin zu rot. Aktuell verwendet GBO Datacomp für die Datenvisualisierung unter
anderem Grafana \footnote{https://grafana.com/}, eine in Golang geschriebene Open Source Software, die zur Analyse
und Überwachung von Daten verwendet werden kann. Gerade bei der Individualisierbarkeit der Visualisierung
von Daten sind bei Grafana kaum Grenzen gesetzt. Versetzt man sich in die Sicht des Kunden, sind Feinjustierungen
an dem Aussehen und der Darstellung der Diagramme für den Kunden selbst von zweitrangiger Bedeutung.
Wichtiger für den Kunden ist, dass er alle relevanten Daten klar strukturiert vor sich hat. Um die Anordnung
der unterschiedlichen Diagramme zu gewährleisten, reicht ein robustes, selbsterklärendes Verfahren aus.
Auf der anderen Seite ist auch genau das ein entscheidender Vorteil von Grafana. Gerade durch die
Individualisierbarkeit in vielen bereichen der Funktionalität der Software, müssen Veränderungen nicht
direkt in der Codebasis vorgenommen werden. Die Flexibilität dieser Arbeit soll sich jedoch auf die
Erstellung neuer Diagramme konzentrieren. Grafana entkapselt bestimmte Teile der Logik der Software
in ausgegliederte Plugins.\cite{GrafanaDeveloperGuide} Die Kosten für zukünftige Änderungen werden
somit gering gehalten. Dieses Qualitätsmerkmal ist ausschlaggebend für eine langlebige Softwarelösung.

Was können wir aus dem Fallbeispiel Grafana lernen? Eigene Logik sollte in austauschbare Komponenten
ausgelagert werden. Konkret heißt das; die Datenvisualisierung muss von der Grundanwendung abstrahiert
werden.

In der Regel wird der Kunde lieber aus einer großen Auswahl an Diagrammen ein passendes aussuchen wollen,
als sich sein eigenes Diagramm zu gestalten. Das Ziel sollte sein, dem Kunden so viel Arbeit wie möglich abzunehmen.
Dabei ist der Auslieferungsprozess von großer Bedeutung. Einzelne Diagramme müssen unabhängig von der Version der
Gesamtanwendung auslieferbar sein. Somit kann auf Kundenwünsche schnell reagiert werden.

\subsection{Codebasis}
\label{subsec:codebasis}
Die Analyse der Anforderungen an die Codebasis ist für die langlebigkeit der Software bedeutsam.
Dabei soll es um die Anforderungen an die Tests, die stetige Auslieferung,
den Einstieg und die Weiterverwendbarkeit, die Erweiterbarkeit und die Wartbarkeit der Software gehen. 

Um die Anforderungen an die Tests zu erschließen, muss zuerst die Anforderung an die verschiedenen
Arten von Tests gestellt werden. Dabei gibt es verschiedene Sichtweisen. Zum einen kann man aus
Sicht des Benutzers testen, also bestimmte Funktionalitäten, die der Benutzer verwenden; desweiteren
kann man aus Sicht der softwaretechnische Zusammenhänge testen; Zuletzt kann man aus der Sicht von
Qualitätsmerkmalen der Software testen.\cite{WikiSoftwaretest} Zum besseren Verständnis hier ein
paar Beispiele: Tests aus Sicht des Benutzers sind End-To-End-Tests, die User Stories nachahmen.
So kann mit Cypress, einem JavaScript-Framework für End-To-End-Tests, GUI-Interaktionen getestet werden.
Tests aus softwaretechnischer Sicht sind meistens Modul- und Integrationstests. Somit kann die reibungslose
Kommunikation zwischen verschiedenen Teilen der Softwarearchitektur sichergestellt werden. Qualitätsmerkmale
von Webseiten können mit Lighthouse, einem Open-Source Werkzeug von Google, getestet werden. \footnote{https://developers.google.com/web/tools/lighthouse}
Lighthouse testet Performanz, Zugänglichkeit, optimale Vorgehensweisen, SEO und die Unterstützung
des Funktionsumfangs von PWAs.

Wo sind Tests zwingend erforderlich und wo sind sie überflüssig? Um diese Frage zu beantworten, muss
man die Vor- und Nachteile, die Tests mit sich bringen, abwägen. Was sind diese Vor- und Nachteile?
Tests sind von grundaus essentiell für die Qualitätssicherung der Software. Allerdings bringen Sie auch
einen Implementationsaufwand mit sich. Dieser ist bei dem Lösen von komplexen Problem um einiges kleiner
als das Lösen des Problems selbst. Ein testgetriebener Ansatz kann sogar den Lösungsvorgang beschleunigen,
da man sich die benötigte Funktionalität bei der Implementierung der Tests verdeutlicht.
Bei kleinen Problemen ist die Implementierung der Tests allerdings komplexer als das Problem selbst.
Hier können Tests den Entwicklungsvorgang bremsen. Dies trifft speziell auf schnelllebige Teile der 
Software zu. So macht es keinen Sinn, im Frontend eine GUI-Komponente zu testen, bei der man sich nicht sicher ist,
ob man diese nicht innerhalb kürzester Zeit wieder verwürft. In Bezug auf die Anforderung an die Testabdeckung dieser Arbeit bedeutet das;
Im Frontend sollten vorerst nur komplexe Problemstellungen getestet werden. Auf Integrationstests und Tests
zu Qualitätsmerkmalen der Anwendung sollte großer Wert gelegt werden. Um die Softwarequalität kontinuierlich
zu sichern, müssen die Tests in den automatisierten Softwareauslieferungsprozess eingebunden werden. 
So sollte kein Merge auf den Master möglich sein, wenn nicht die CI-Pipeline fehlerfrei durchlaufen wurde.
Zu den zuvor als relevant definierten Testarten gilt die Aussage aus dem Buch Clean Code von Robert C. Martin:
"Die Tests sind solange unzureichend, wie es Bedingungen gibt, die nicht von den Tests geprüft werden,
oder Brechnungen, die nicht validiert werden." \cite[S. 370]{CleanCode}

Um die Anwendung in einer realen Umgebung zu Testen, benötigt es die Methoden CI/CD
zu praktizieren. Somit ist es möglich, eine Phase der Pipeline der Qualitätssicherung zu widmen.
So können Product Owner die Software vor der Auslieferung nochmal überprüfen. Außerdem
entlastet CI/CD den Rechner der Entwickler, da so die Tests automatisiert auf einem Server
ausgeführt werden. Natürlich werden bestimmte Tests, wie beispielsweise Modultests, auch 
lokal vor dem Einchecken neuer Softwareänderungen ausgeführt, aber eben nicht alle. Gerade
bei einer Microservice-Infrastruktur sind Integrationtests und somit die Methoden CI/CD essentiell.

Der Einstieg für neue Entwickler sollte so unkompliziert wie möglich gestaltet sein. Um dem neuen
Entwickler den Eintstieg in das Projekt zu erleichtern, sollte die Dokumentation gut gepflegt sein.
Desweiteren ist das Aufsetzen des Projektes soweit zu automatisieren, dass mit so wenig Befehlen
wie möglich die Entwicklungsumgebung zum Laufen gebracht werden kann.

Die Erweiterbarkeit der Software ist nur durch eine gut durchdachte Softwarearchitektur erreichbar.
So muss die Logik der Gesamtanwendung klar getrennt und Schnittstellen klar definiert werden. Es sollte
immer damit zu rechnen sein, dass die einzelnen Komponenten, die die Gesamtsoftware bilden, jederzeit
ausgetauscht werden können. Wird eine einzelne Komponente zu groß, muss diese anhand ihrer Logik in weitere
Komponenten aufgeteilt werden. Es muss stetig damit gerechnet werden, dass neue Funktionalitäten im
Laufe der Zeit hinzugefügt werden können.

Für die Wartbarkeit der Software ist ein aussagekräftiges Logging in alle Services aufzunehmen.
Damit die Entwickler bei auftauchenden Problemen schnell und effizient über die Ursache informiert werden,
müssen mehrere Loglevel implementiert werden. Im falle einer Microservice-Infrastruktur sollte ein Überwachungssystem
integriert werden, mithilfe dessen die einzelnen Services kontrolliert werden können.
