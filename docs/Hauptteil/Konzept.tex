\chapter{Konzept}
\label{chap:konzept}
\setcounter{footnote}{0}
Dieses Kapitel ist aus der Sicht des Datenflusses organisiert. Als erstes
gilt es die Daten zu beschaffen. Daraufhin werden sie verarbeitet und
der Frontendanwendung zugeliefert. Dort werden die Daten den Dashboards
und deren Diagrammen zugewiesen. In den Diagrammen werden die Daten
visualisiert. Zu guterletzt werden die Einstellungen der Dashboards
und die Zuweisung der Daten gespeichert.

Der zuvor beschriebene Vorgang geht bereits davon aus, dass die Daten im Backend
verarbeitet und daraufhin dem Frontend ausgeliefert werden. Die Arbeit entscheidet
sich hierfür aus drei Gründen. Sicherheit, Performanz und Flexibilität. Die URLs
zum Abrufen der Daten sollen im Client einstellbar sein. Würde man die Anfragen an
die externen APIs zur Beschaffung der Daten direkt aus dem Client heraus senden,
müssen bei allen externen APIs CORS-Header für die eigene Origin gesetzt sein\cite{CORSW3C}.
Desweiteren bieten sich serverseitige Programmiersprachen besser zur Verhinderung
von Attacken zur Einschleusung von schädlichem Quellcode an.\footnote{Bei
Unmarshalling- und Validierungsverfahren ist Golang flexibler als das im Client ausgeführte JS.}
Für die Performanz sprechen folgende Gründe: Das Endgeräte des Clients ist womöglich
nicht zur Datenverarbeitung ausgelegt. Das mehrfache Berechnen der gleichen Anfrage
kann im Backend einfach abgefangen und aus dem Cache genommen werden. Die Verbindung
des Backendservers zur externen API ist in der Regel schneller als die Verbindung
des Clients zum Server. Wenn das Backend die Daten bereits Verarbeitet, wird nur noch
eine verdichtete Form der Daten an den Client gesendet.\footnote{Es ist davon auszugehen, dass
die aus der Verarbeitung resultierende Datenmenge geringer als die der initialen Anfrage ist.}
Für die Flexibilität spricht, dass die Verarbeitung so nicht in neue Frontendanwendungen
integriert werden muss.

Die Benutzerverwaltungslogik soll klar von der Datenbeschaffungs-, Verarbeitungs-
und Zulieferungslogik getrennt sein. Die Arbeit geht somit von zwei Services aus.
Dem \code{Resource Management Service}, der für die Verwaltung von Ressourcen wie Dashboards,
Diagrammen, Datenquellen und Benutzer zuständig ist, und dem \code{Data Delivery Service},
der sich um die zu analysierenden Daten kümmert.

Die folgenden Abschnitte gehen genauer in das Konzept der Gesamtanwendung ein.
In \Cref{sec:beschaffung} geht es um die Beschaffung der Daten. In \Cref{sec:verarbeitung}
wird ein Konzept zur Transformation der Daten in ein für die Diagramme plausibles Format
erarbeitet. In \Cref{sec:zulieferung} geht es um ein Konzept, das
eine leistungsfähige Datenauslieferung an die Clients ermöglicht. In \Cref{sec:andordnungundzuweisung}
geht es um die Erstellung Eines Dashboards und dessen Zuweisung auf die
Datenquellen. In \Cref{sec:darstellung} wird ein Konzept entwickelt,
was Daten möglichst flexibel im Client visualisieren kann.
Zu guterletzt wird in \Cref{sec:speicherung} die Speicherung der Dashboards behandelt.

\section{Beschaffung}
\label{sec:beschaffung}
Die Datenbeschaffung selbst behandelt das Verfahren, zu analysierende Daten
von einer externen API zu dem \code{Data Delivery Service} zu transportieren.
Hier bieten sich Open Source Message Broker wie RabbitMQ und Apache Kafka an.
Aktuell stellt GBO Datacomp hierfür allerdings eine REST-API zur Verfügung.
Für den Datentransport von der REST-API zu dem \code{Data Delivery Service} reicht somit
eine GET-Anfrage aus.

\section{Verarbeitung}
\label{sec:verarbeitung}
Die Verarbeitung der Daten ist ein komplexes Verfahren. Die bereits in der
Anforderungsanalyse erwähnt, liefert die API von GBO Datacomp die Daten im
JSON-Format aus. Die Arbeit entscheidet sich zur Datenverarbeitung für die
Verwendung einer Abfragesprache. In \Cref{chap:analysebestehendersoftwareloesungen}
wurden drei bestehende Softwarelösungen im Bereich der Datenvisualisierung von
Betriebsdaten analysiert. Dabei verwendete jede der Lösungen eine Abfragesprache,
um komplexere Datentransformationen durchzuführen. Bei der Implementation einer
GUI zur Definition der Verarbeitungsweise der Daten kommt man schnell an seine Grenzen.
Die Möglichkeit, Felder aus verschiedenen Ebenen einer JSON-Datei auszuwählen
und in eine gewollte Ausgangsstruktur zu transformieren, ist mithilfe einer GUI nahezu
unmöglich. Desweiteren ist mit einer Abfragesprache auch um einiges schneller als mit
der Verwendung einer GUI. Allerdings benötigt die Verwendung einer Abfragesprache
Einarbeitungszeit. Nichtsdestotrotz ist es für die Arbeit die einzige Möglichkeit,
die Flexibilität für die Datenverarbeitung sowie die Datenmodellierung der Daten
beizubehalten.

Softwarelösungen wie Power BI, Qlik und Tableau verwenden SQL-ähnliche Abfragesprachen.
Da die Eingangsdaten im JSON-Format vorliegen und das Frontend in JS geschrieben wird,
ist es naheliegend, eine JSON-Abfragesprache für die Arbeit zu verwenden. Die Arbeit
verwendet hierfür die JSON-Abfragesprache \code{JMESPath}.\footnote{http://jmespath.org/}
Die Abfragesprache wurde von James Saryerwinnie ins Leben gerufen und wird aktuell von
Amazon in ihrer Amazon Web Services CLI verwendet.\cite{AWSJMESPath} Eine Umsetzung von
\code{JMESPath} gibt es in Golang als auch in JS. Somit ist es möglich, die Daten im
Frontend sowie Backend zu verarbeiten. Die Idee ist es, den Benutzern die Verarbeitung
im Frontend zu ermöglichen. Dort kann der Benutzer die Daten passend verarbeiten und die
gewollte Abfragezeichenfolge abspeichern. Diese wird dann bei der Darstellung des 
Dashboards in der Datenanfrage an die \code{Data Delivery API} mitgesendet.
Die weitere Verarbeitung findet dann im Backend statt.

\begin{listing}[]
    \inputminted{jsx}{snippets/json/jmespath-example/jmespathinput.json}
    \caption{JMESPath Eingangsdaten}
    \label{lst:jmespatheingangsdaten}
\end{listing}

\begin{listing}[]
    \inputminted{jsx}{snippets/json/jmespath-example/jmespathquery.txt}
    \caption{JMESPath Abfrage}
    \label{lst:jmespathabfrage}
\end{listing}

\begin{listing}[]
    \inputminted{jsx}{snippets/json/jmespath-example/jmespathoutput.json}
    \caption{JMESPath Ausgangsdaten}
    \label{lst:jmespathausgangsdaten}
\end{listing}

Um die Datenverarbeitung mithilfe von \code{JMESPath} zu verdeutlichen, folgt ein
Beispiel: Das Array in Quellcode \ref{lst:jmespatheingangsdaten} beinhaltet zwei Ressourcen;
\code{Maschine 1} und \code{Maschine 2}. Es repräsentiert die durch die REST-API empfangenen
Daten.\footnote{Die Beispieldaten wurden simplifiziert.} In Quellcode \ref{lst:jmespathabfrage}
sieht man eine Abfrage, anhand welcher die Daten Verarbeitet werden. In Quellcode \ref{lst:jmespathausgangsdaten}
sieht man das Endresultat der Abfrage. Die Werte der Felder \code{name}, \code{performance} und \code{quality}
wurden mithilfe des Array-Selektor \code{[]} und des angehangenen Feldnamen ausgewählt.
Das Format der Datenausgabe ist ein gültiger Datensatz für die Diagrammvisualisierungsbibliothek
\code{chart.js}. Somit lassen sich die Eingangsdaten mithilfe eines sehr flexiblen Verfahrens
in verwendbare Ausgabedaten transformieren.

\section{Zulieferung}
\label{sec:zulieferung}
Das Konzept der Zulieferung der Daten ist in drei Abschnitte
unterteilt. In \Cref{subsec:datenuebermittlungskonzept} wird das Datenübermittlungskonzept
zwischen dem Client und dem \code{Data Delivery Service} vorgestellt.
In \Cref{subsec:echtzeitauslieferung} geht es um die Liveübertragung der Daten. In
\Cref{subsec:caching} wird ein Konzept zur Leistungssteigerung der Datenauslieferung
erarbeitet.

\subsection{Datenübermittlungskonzept}
\label{subsec:datenuebermittlungskonzept}
Zur Datenübermittlung verwendet die Arbeit das Publish/Subscribe Architekturkonzept.
Um die bidirektionale Kommunikation zwischen dem Client und dem Server zu ermöglichen,
wird das WebSocket-Protokoll verwendet. Verglichen mit HTTP ist das WebSocket-Protokoll
aufgrund der fehlenden Header-Fixkosten performanter. Eine aufrechterhaltene WebSocket-Verbindung
benötigt allerdings auch mehr Rechenleistung als einzelne Mitteilungen über HTTP. Desweiteren
kann HTTP nur bedingt mithilfe von Server-Sent Events eine bidirektionale Verbindung aufbauen.
Server-Sent Events selbst sind allerdings nur unidirektional\cite[2. Abschnitt]{WebSocketVSSSE}.

\begin{figure}
    \begin{center}
    \includegraphics[scale=0.2]{img/abbildungen/Verteilung}
    \end{center}
    \caption{Verringerung der Redundanz des Datenflusses}
    \label{figure:uebersichtderdatenauslieferung}
\end{figure}

Das Konzept der Datenübermittlung ist folgendes: Besucht der Nutzer ein Dashboard,
abonnieren die einzelnen Diagramme die jeweiligen Kanäle, die die Datenquellen 
repräsentieren (Siehe Abbildung \ref{figure:uebersichtderdatenauslieferung}).
Desweiteren wird eine Nachricht an den \code{Data Delivery Service} veröffentlicht,
indem dieser darüber informiert wird, dass der Client jetzt bereit ist Daten zu empfangen.
Je nach angefragtem Interval sendet nun der \code{Data Delivery Service} verarbeitete
Daten an das Frontend, die in der Frontendanwendung angezeigt werden. Um die
Redundanz im Datenfluss zu verringern, werden die ausgehenden Nachrichten
so gefiltert, dass für jede Datenquelle nur eine Nachricht versendet wird. Die ankommenden
Nachrichten werden hingegen so verteilt, dass alle Diagramme mit der passenden Datenquelle
versorgt werden.

\subsection{Echtzeitauslieferung}
\label{subsec:echtzeitauslieferung}
Durch das Publish/Subscribe Architekturkonzept ist die Grundlage für die Echtzeitauslieferung
bereits geschaffen. Die im Dashboard enthaltenen Diagramme warten auf ankommende Nachrichten,
mit derer enthaltenen Daten sie ihre Visualisierung neuzeichnen. Der \code{Data Delivery Service}
benötigt eine in einem separaten Prozess taktierte Ausführung einer Routine, die die Daten
anhand eines gegebenen Intervals anfragt, verarbeitet und an die Frontendanwendung sendet.

\subsection{Caching}
\label{subsec:caching}
Der initiale Lade-, Verarbeitungs- und Auslieferungsprozess kann eine für den Benutzer
unangenehme Zeit in Anspruch nehmen. Um das initiale Warten beim Laden eines Dashboards
zu verringern, bietet sich folgende Caching-Strategie an: Bei einer Anfrage schaut
der \code{Data Delivery Service} zuerst, ob es bereits einen Cache-Eintrag für die
angefragte ID der Datenquelle gibt. Existiert dieser, wird eine sofortige Nachricht
mit den Daten aus dem Cache an die Frontendanwendung geschickt. Existiert anfangs noch
kein Cache-Eintrag, wird dieser Schritt übersprungen. Daraufhin werden
die aktuellen Daten in einem separaten Prozess abgefragt, verarbeitet, der Cache
aktualisiert und eine zweite Nachricht mit den aktuellen Daten an die Frontendanwendung
geschickt. Der Key für den Cache-Eintrag soll hierbei aus der ID der Datenquelle sowie
einem aus dem Request-Body generierten Hash zusammengesetzt werden. Somit werden
Kollisionen zwischen unterschiedlichen Benutzern ausgeschlossen\footnote{Die ID der Datenquellen ist
unter allen Nutzern einzigartig und wird dem Ersteller der Datenquelle zugewiesen.}
und Änderungen in dem Request-Body erkannt. Kollisionen unterhalb verschiedener
Benutzer könnten fatale Folgen haben. Im schlimmsten Fall würden so geheime Daten
in die Hände eines fremden Nutzers gelangen. Daher ist die Integration der Datenquellen-ID
in den Cache-Key essentiell.

\section{Anordnung und Zuweisung}
\label{sec:andordnungundzuweisung}
Das Konzept der Anordnung und Zuweisung ist in zwei Abschnitte unterteilt.
In \Cref{subsec:diagrammanordnungsverfahren} geht es um die Konzeptentwicklung
der Anordnung der Diagramme innerhalb eines Dashboards. Daraufhin geht
es in \Cref{subsec:zuweisungungvondatenquellen} um die Zuweisung der Datenquellen
zu den angeordneten Diagrammen.

\subsection{Diagrammanordnungsverfahren}
\label{subsec:diagrammanordnungsverfahren}
Das Diagrammanordnungsverfahren beschreibt den Prozess, einzelne Diagramme in einem Dashboard anzuordnen.
Um das Verfahren zu vereinfachen, ist ein automatisches Anordnen der Diagramme diagonal sowie vertikal vorgesehen.
Die Anordnung der einzelnen Diagramme soll mithilfe der FlexBox Technologie implementiert werden.
\footnote{CSS Flexible Box Layout, bekannt als FlexBox, ist ein standardisiertes Anordnungsmuster, das von allen gängigen Browsern unterstützt wird.\cite{CanIUseFlexBox}}
Im mobilen Format sollen die einzelnen Diagramme untereinander aufgelistet werden. 

\begin{figure}
    \begin{center}
    \includegraphics[scale=0.2]{img/abbildungen/DiagrammanordnungsverfahrenMitLegende}
    \end{center}
    \caption{Diagrammanordnunsverfahren}
    \label{figure:diagrammanordnungabbildung}
\end{figure}

Die Diagramme sollen nach und nach per Drag and Drop in das Dashboard integriert werden. Dieses Verfahren
wird in Abbildung \ref{figure:diagrammanordnungabbildung} dargestellt. \(a\) beschreibt den
initialen Zustand eines leeren Dashboards. Hier kann man durch Drag and Drop das erste Diagramm dem Dashboard
zuordnen. \(b\) beschreibt den Zustand des Dashboards mit genau einem enthaltenen Diagramm. Hier kann man das
neue Diagramm in alle Richtungen einfügen. Dabei Teilt das bestehende Diagram den Platz mit dem neuen gleichmäßig
auf. Bei der mittigen Positionierung wird das vorhandene Diagramm durch das neue ersetzt. \(c\) und \(d\)
beschreiben den Zustand, bei dem sich mehr als ein Diagramm im Dashboard befindet. Wenn man im Zustand \(b\)
ein Diagramm oberhalb oder unterhalb einfügt, bilden die beiden Diagramme eine Spalte. Beide dieser Diagramme
beinhalten nun den Zustand \(c\). Additional zu den möglichkeiten von Zustand \(b\), kann hier ein Diagramm
auch dazwischen gefügt werden. Es teilt sich dann den Platz mit den anderen Diagrammen in der Spalte.
Fügt man bei Zustand \(b\) ein Diagramm links oder rechts ein, bilden die beiden Diagramme eine Reihe.
Beide dieser Diagramme befinden sich nun in Zustand \(d\). Additional zu Zustand \(b\) können hier nun
auch neue Diagramme in der gleichen Reihe positioniert werden. Diese Reihen und Spalten können sich
immer weiter verschachteln. Somit kann der Benutzer mit einem ziemlich einfachen, intuitiven Verfahren
das Dashboard mit Diagrammen füllen. Durch einen Button in der Ecke rechts oben im Diagramm-Fenster
können diese auch wieder gelöscht werden. Dabei verhalten sich die zuvor beschriebenen Verfahren rückläufig.

\subsection{Zuweisung von Datenquellen}
\label{subsec:zuweisungungvondatenquellen}



\section{Darstellung}
\label{sec:darstellung}

\subsection{Pluginverfahren}
\label{subsec:pluginverfahren}
% Bündelung der Bibliotheken

Die Arbeit verfolgt auch für das Frontend einen Microservice-Ansatz. Man spricht hier von einem
Microfrontend. Michael Geers schreibt dazu in einem Artikel auf Digital Pioneers folgendes:
"Der Ansatz, der sich jetzt förmlich aufdrängt, ist die Umsetzung der Microservice-Idee
auch im Frontend. Anstatt einer ­großen Single-Page-Applikation baut man mehrere unabhängige
Teilapplikationen, die miteinander kommunizieren. Diese sind dann deutlich einfacher zu verstehen.
Der Einsatz einer neuen Technologie lässt sich in einem kleineren Rahmen testen."\cite{MicrofrontendT3N}
Dabei sollen die Diagramme eines Dashboards als einzelne Services ausgelagert werden.
Die Diagramme können so einzeln entwickelt und ausgeliefert werden. Aufgrund der
gleichen Machenschaft der Schnittstellen der Diagramme, kann man diese Art des Code-Splittings auch als
einen Plugin-Ansatz ansehen. Die Diagramme sollen über die In-Memory-Datenbank
Redis zur Verfügung gestellt werden. Die Diagramme werden mithilfe
von Dynamic Imports\footnote{Dynamic Imports gibt es seit ES6 (ECMAScript 2015)\cite{DynamicImportsV8}}
während der Laufzeit agil nachgeladen. Da immer nur die verwendeten Diagramme aus der In-Memory-Datenbank
geladen werden, kann die Webanwendung eine nahezu unendliche Auswahl an möglichen Diagrammen zur Visualisierung
der Daten bereitstellen.

\section{Speicherung}
\label{sec:speicherung}

\subsection{Aufbau des Dashboards}
\label{subsec:aufbaudesdashboards}
Die Anordnung der Diagramme wird als Baumstruktur im JSON-Format in der Datenbank persistiert.

\subsection{Datenquellen und Einstellungen}
\label{subsec:datenquellenundeinstellungen}
