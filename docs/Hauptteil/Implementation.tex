\chapter{Implementation}
\label{chap:implementation}
In diesem Kapitel geht es um die Umsetzung der in \Cref{chap:konzept} erarbeiteten
Konzepts. Da die Arbeit einen CI/CD-Ansatz verfolgt, muss vor der Implementation
der einzelnen Services die Infrastruktur und die Pipeline umgesetzt werden. 
Wo Tests benötigt werden, werden diese vor der Realisierung der einzelnen
Komponenten verwirklicht. Aufgrund dieser Erkenntnisse, ist das Kapitel der
Implementation wie folgt gegliedert: In \Cref{sec:infrastruktur} geht es um
die für die Entwicklung, die Testphase sowie die in Produktion laufende Anwendung
benötigten Server. In \Cref{sec:gesamtsysten} geht es um den Aufbau der gesamten
Anwendung. In \Cref{:sec:projektaufbau} wird der Aufbau des Projekts behandelt.
In \Cref{sec:pipeline} wird die Implementation der stetigen Erstellung, Testung und
Auslieferung der Gesamtanwendung beschrieben. In den folgenden Abschnitten wird die
Implementation der von der Anforderungsanalyse geforderten Features erläutert.
In \Cref{sec:frontendanwendung} wird die Umsetzung der progressiven Webanwendung
mit dem Frontendframework React geschildert. In \Cref{sec:plugins} geht es um die
Implementation der für die Dashboards benötigten Plugins, die die Daten
in Diagrammen visualisieren. In \Cref{sec:resourcemanagementservice} wird
die testgetriebene Entwicklung der für die Benutzerverwaltung benötigte REST-API
dargestellt. Zuallerletzt wird in \Cref{sec:datadeliveryservice} die Implementation
des Services geschildert, welcher die Frontendanwendung mit den Daten versorgt.

\section{Infrastruktur}
\label{sec:infrastruktur}
Für die Entwicklung der Gesamtanwendung benötigt die Arbeit einen Linux-Server,
auf dem Gitlab CE, Gitlab Runner und eine Docker Registry
installiert sind. Des Weiteren benötigt es einen Server für die Testphase und die
Produktion. Die einzelnen Services werden mit Docker Swarm\footnote{https://docs.docker.com/engine/swarm/},
einem Orchestrierungssystem für Docker-Container, auf dem Produktivsystem ausgeführt. Das Ziel
für die Zukunft ist es, multiple Instanzen der Services auf mehreren Linux-Servern zu verteilen.
Dies soll mithilfe des Open Source IAC-Werkzeugs\footnote{IAC steht für Infrastructure as Code}
Terraform über eine Cloudanbieter-API und dem Orchestrierungssystem Kubernetes automatisiert werden.
Die Arbeit beschränkt sich auf drei Linux-Server. Einen für die Versionsverwaltung
und die CI/CD-Prozesse, einen für die Testphase und einen für das Ausführen der Anwendung in Produktion.
Die drei Server werden von Hetzner, einem Cloudanbieter, bereitgestellt und haben Ubuntu 18.04
vorinstalliert. Der Server für die CI/CD-Prozesse und die Versionsverwaltung besitzt 4 VCPUs, 
16GB RAM und  haben 4 VCPUs, 16GB RAM und 160GB SSD-Speicher.\footnote{Im Regelfall handelt es sich bei
den VCPUs um Intel Skylake Xeon CPUs mit 2.1 GHz.\cite{CPUusedInHetznerCLoud}}
Der Server der Testphase besitzt 2 VCPU, 8GB RAM und 80GB SSD-Speicher. Der für die Produktion
ausgelegte Server besitzte 8 VCPUs, 32GB RAM und 240GB SSD-Speicher.
Neben den Servern benötigt es noch Volumes, um die Daten der Datenbanken zu persistieren.
Die an die Server angebrachten Volumes werden mithilfe von Docker auf die für die
Beständigkeit relevanten Ordner der Datenbank-Container zugewiesen. Die Zuweisung variiert
je nach Datenbank.

Um die Server über das Internet zu erreichen, benötigt es Domains
und Subdomains, die mithilfe von DNS-Records auf die vom Server bereitgestellten
IPv4- und IPv6-Adressen verweisen.\footnote{Man kann die Server natürlich auch direkt über
die IP-Adresse ansprechen.} Die Domain \code{blicc.org} dient als
Hauptdomain und wird in der Produktion verwendet. Der Name Blicc
wurde zur Repräsentation des Produktes gewählt. Bei einer Verwendung der Anwendung
als White-Label-Produkt wird der Name durch den der zu repräsentierenden Firma ersetzt.
Der Name Blicc ist eine veränderte Form des deutschen Wortes "Blick". Der Name wurde
daher verändert, da die Verfügbarkeit von "Blick" in den meisten Domainvariationen
nicht mehr erhältlich war oder aber überteuert angeboten wurde. Die Domain für den
Server der Testphase ist \code{testing-stage.org}. Die beiden Domains besitzten die
Subdomains \code{api} und \code{delivery}. Die Subdomain \code{api} verweist auf
den Resource Management Service; die Subdomain \code{delivery} auf die Data Delivery Service.
Außerdem gibt es die \code{monitor} Subdomain, welche auf die Kontrollübersicht von Traefik
verweist. Traefik dient als Reverse-Proxy und Load-Balancer, bietet zusätzlich allerdings
auch noch geringe Überwachungsfunktionalitäten in einem webbasierten Dashboard an.
Dazu mehr in \Cref{subsec:reverseproxyundloadbalancer}.

Die Docker Registry ist unter der Domain \code{registry.thiloilg.com}, Gitlab CE
unter \code{gitlab.thiloilg.com} erreichbar. Die Repositories werden alle zusätzlich
auf Github gespiegelt. Github dient als drittes Backup, falls die Repositories auf dem
eigenen Gitlab Server und dem lokale Rechner verloren gehen. Mit dem Git-Befehl in
Quellcode \ref{lst:gitalias} kann ein Alias gesetzt werden, um zu allen Remote Repositories
gleichzeitig zu pushen. Der senkrechte Strich im Linux Terminal, bekannt als "Pipe",
trennt zwei Befehle in der Kommandozeile. Das Ergebnis des ersten Befehls wird
an den zweiten Befehl weitergereicht.

\begin{listing}
    \inputminted{sh}{snippets/sh/pushall.sh}
    \caption{Konfiguration eines eigenen Git Alias}
    \label{lst:gitalias}
\end{listing}

Für das Aufsetzen der Server benötigt man drei Schritte.
Als erstes verbindet man sich über SSH zu dem Server und richtet eine Firewall ein.
Hierfür verwendet die Arbeit UFW, eine in Ubuntu vorinstallierte
Firewall.\footnote{UFW steht für Uncomplicated Firewall} Dabei öffnet man alle Ports,
die von den Anwendungen des Server benötigt werden. In der Regel sind das Port 80
für HTTP und Port 443 für HTTPS. Aufgrunddessen, dass bei dem Gitlab CE Server
Port 22 bereits für die SSH verbindung benötigt wird, musste der Gitlab Shell SSH
Port auf 2222 verlegt werden. Als zweiten Schritt Installiert man alle nötigen
Programme auf dem Server. Das wichtigste Programm ist Docker CE. In Schritt
drei kopiert man mithilfe von SCP die \code{docker-compose.yml} Datei auf den
Server und startet Docker Swarm mit \code{docker swarm init}. Daraufhin wird
ein Stack mit den in der Docker-Compose Datei definierten Services aufgezogen.
Dieser Prozess ist für den Test- sowie den Produktionsserver in der CI/CD-Pipeline
automatisiert. Mehr dazu in \Cref{sec:pipeline}.

\section{Gesamtsystem}
\label{sec:gesamtsysten}
In diesem Abschnitt wird aus dem Konzept ein komplettes System
entwickelt. Der Abschnitt fokussiert sich auf die Merkmale der Komponenten,
die für das Gesamtsystem relevant sind. In \Cref{subsec:systemuebersicht} verschafft
sich die Arbeit einen groben Überblick über das System. Anhand einer Grafik wird das Zusammenspiel
der einzelnen Komponenten erläutert. In  \Cref{subsec:reverseproxyundloadbalancer}
wird der Reverse Proxy und der Load Balancer behandelt. In \Cref{subsec:inhaltsauslieferung} wird
die Inhaltsauslieferung vorgestellt. Dabei stellt sich die Frage,
wie die Ressourcen möglichst performant an die Clients transportiert
werden können. In \Cref{subsec:wahlderdatenbankarten} geht es um die Wahl
der Datenbankart der einzelnen Services. Abschließend setzt sich die Arbeit
in \Cref{subsec:ueberwachungundwartung} mit der Überwachung und Wartung
des Systems auseinander.

\subsection{Systemübersicht}
\label{subsec:systemuebersicht}
Die Abbildung \ref{figure:uebersichtueberdassystem} verschafft einen guten Überblick
über das Zusammenspiel der einzelnen Komponenten. Im oberen Bereich sieht man die
Frontendanwendung sowie eine Repräsentation einer externen API. Die Frontendanwendung
besteht aus dem Frontend sowie dem integrierten Plugin-System. Je nach Kommunikationsart
unterhält sich das Frontend über HTTPS oder WSS mit dem Backend. Die HTTPS-Kommunikation
wird durch einen Service Worker interferiert, um das Frontend-Caching und die
Offline-Funktionalität zu ermöglichen. Das WebSocket-Protokoll verbindet das Frontend
über direktem Web mit dem Backend. Dies kann allerdings von Browser zu Browser variieren.
Mache Versionen der Browser enthüllen die WebSocket-Kommunikation im SW, andere
nicht.\footnote{Siehe Diskussion auf Github, W3C Service Worker \cite{GithubIssueWebSocketExpose}}
Die Kommunikation zwischen externen APIs und dem Backend kann über HTTP als auch
HTTPS erfolgen. Zukünftige Kommunikationen über WebSockets oder QUIC\footnote{QUIC (Quick UDP Internet Connections) ist ein von Google entwickeltes, auf UDP basierendes Transportprotokoll.\cite{IETFQUICWhatsHappening} Die IETF arbeitet an einer Standardisierung des Internetprotokolls.\cite{DatatrakcerIETFQuic}}
sind denkbar.

\begin{figure}
    \begin{center}
    \includegraphics[scale=0.2]{img/abbildungen/MicroserviceInfrastruktur}
    \end{center}
    \caption{Übersicht über das System}
    \label{figure:uebersichtueberdassystem}
\end{figure}

Das Backend und dessen Microservice-Infrastruktur wird durch einen Reverse Proxy vom Internet versteckt.
In dieser Zwischenschicht befinden sich unter anderem ein Load Balancer sowie ein Monitoring System.
Das Backend ist in drei Services unterteilt; den Data Delivery Service, den Resource Management Service
und den Content Delivery Service. Der Data Delivery Service sowie der Resource Management Service sind jeweils
in drei Container unterteilt. Der wichtigste Container führt die Servicelogik aus. In der zweiten
Schicht befindet sich eine In-Memory-Datenbank, die für das Caching verantwortlich ist. Zuallerletzt
kommt eine Datenbank, um den Fortbestand der Daten zu sichern. Die Caching-Strategie sowie die Art der
Datenbank variiert je nach Anforderung an den Service. Für die Langzeitspeicherung der Daten wird je nach Service
eine dokumentenorientiertes als auch ein relationales Datenbanksystem verwendet. Der Content Delivery Service
ist für die Auslieferung der statischen Dateien des Frontends verantwortlich.

\subsection{Reverse Proxy und Load Balancer}
\label{subsec:reverseproxyundloadbalancer}
Anders, als bei einem Proxy, der den Client von dem Server versteckt, versteckt ein Reverse Proxy
eine Serverinfrastruktur vor dem Client. Durch die Anonymisierung der Serverinfrastruktur
sind Angriffe gegen spezifische Servertechnologien erschwert, da deren Schnittstelle
nicht direkt mit der Außenwelt kommuniziert. In der Regel übernimmt der Reverse Proxy
auch die SSL/TLS-Verschlüsselung. Des Weiteren kann er auch zur Verteilung der Serverlast,
also als Load Balancer verwendet werden. Ein typisches Beispiel für einen Reverse Proxy
ist ein Nginx Webserver. Die Arbeit benutzt als Reverse Proxy und Load Balancer die
in Golang geschriebene Open Source Software Traefik.\footnote{https://docs.traefik.io/}
Traefik unterstützt die Kommunikation mit Orchestrierungssystemen wie Kubernetes
und Docker Swarm, und bietet additional auch noch automatisierte SSL/TLS-Verschlüsselung
mithilfe von Let's Encrypt.\footnote{https://letsencrypt.org/} Laut einer Google Checklist,
ist eine Verbindung über HTTPS Vorraussetzung für eine PWA.\cite{GooglePWAChecklist}
Ohne SSL/TLS-Verschlüsselung werden Funktionalitäten wie das hinzufügen der App zum Homescreen
von den Browsern nicht bereitgestellt. Verschlüsselungsschichten wie TLS und der Vorgänger
SSL liegen zwischen der TCP-Schicht und HTTP/WS. Neben dem Public-Key-Verschlüsselungsverfahren
muss für eine TLS-Verschlüsselung auch die Authentizität des Servers, mit dem kommuniziert wird,
bestätigt sein. Hierfür wird ein EV-TLS-Zertifikate\footnote{EV steht für Extended Validation} benötigt.
Mithilfe von Let's Encrypt, einer gemeinnützigen Zertifizierungsstelle, werden EV-TLS-Zertifikate
kostenlos über eine API zur Verfügung gestellt. Die Arbeit hat sich unter anderem für Traefik entschieden,
da der Reverse Proxy einen automatisierten Vorgang bereitstellt, indem EV-TLS-Zertifikate
mithilfe von Let's Encrypt generiert werden.

Der Load Balancer entscheidet, wie die Serverlast unter den verschiedenen Services aufgeteilt wird.
Die Arbeit verwendet hier das über Docker Swarm bereitgestellte Lastverteilungsverfahren Round-Robin DNS,
eine rotierende Verteilung der eingehenden Anfragen auf unterschiedliche IP-Adressen.\cite{CloudflareRoundRobinDNS} 
Da die Aufrechterhaltung der WebSocket-Verbindungen des Data Delivery Services gegenüber
den Clients allerdings nicht linear verläuft,\footnote{Manche Benutzer beenden die Anwendung sofort, andere lassen Sie über Tage laufen.}
empfiehlt es sich für die Zukunft, die Implementierung eines Lastverteilungsverfahrens
basierend auf Health-Checks zu verwenden.

\subsection{Inhaltsauslieferung}
\label{subsec:inhaltsauslieferung}
Die Inhaltsauslieferung ist dafür zuständig, statische Inhalte der Frontendanwendung
an den Client auszuliefern. Die statischen Inhalte sind unter anderem
HTML-, CSS- und JS-Dateien, JSON-Dateien wie das Web App Manifest sowie Bilddateien.
Die Arbeit verwendet als Webserver Nginx. Steve Souders betont
in einem Vortrag über die Performance von JavaScript, dass ein Drittel der im Browser
geladenen First-Party-Scripts zwischen 90- und 100Kb nicht komprimiert werden.
Das liege daran, dass jQuery, eine der meistgenutztesten JS-Bibliotheken,
unkomprimiert circa bei 100Kb angesiedelt sei.\cite{SteveSoudersMakeJavaScriptFaster}
Laut einer Statistik von Build With benutzten 88,07\% der Top 10k meinstbesuchtesten
Webseiten des Internets jQuery.\footnote{Stand Februar 2020.\cite{BuildWithjQuery}}

Für die kompression der statischen Inhalte verwendet die Arbeit Brotli.
Brotli ist eine von Zoltán Szabadka und Jyrki Alakuijala entwickeltes Kompressionsverfahren,
dass auf die Entropiekodierung Huffman und dem verlustlosen
Datenkompressionsverfahren LZ77 basiert.\cite{BrotliGoogleOpenSourceBlog}
Als Fallback wird Gzip verwendet. Standartmäßig unterstützt Nginx
das Kompressionsverfahren Gzip. Die Arbeit fügt dem Nginx-Dockerfile ein Script
hinzu, welches Brotli installiert. Außerdem wurde die \code{nginx.conf} Datei
so angepasst, damit Nginx Brotli unterstützt.

\subsection{Wahl der Datenbankarten}
\label{subsec:wahlderdatenbankarten}
Der Data Delivery Service verwendet ein dokumentenorientiertes Datenbanksystem.
Hauptgrund hierfür ist die Tatsache, dass man nicht im Vorhinein weiß, wie das
Datenschema auszusehen hat. Der Benutzer der Anwendung kann jegliche API
auswählen, Daten abfragen und diese mithilfe einer JSON-Abfragesprache verarbeiten. Das Datenschema
kann sich also während der Laufzeit des Programms verändern. Im Gegensatz hierzu steht der Resource
Management Service, welcher auf eine relationale Datenstruktur aufgebaut ist. Die Schemata formen das
Erscheinungsbild der API und werden als CRUD-Operationen über eine REST-Schnittstelle enthüllt.
Es stellt sich die Frage, wieso der Resource Management Service nicht auch eine NoSQL-Datenbank verwendet.
Clemens Gull bringt die Kompromisse, welche man bei der Entscheidung zwischen einem relationalen
und einem dokumentenorientierten DBMS eingehen muss, in seinem Buch "Wev-Applikationen
entwickeln mit NoSQL" auf den Punkt:

\begin{quote}
"Da RDBMS (relationale Datenbankmanagementsysteme, also konventionelle Datenbanken) sehr streng
auf die Konsistenz der Daten achten, kann es hier zu Problemen mit der Performance und Verfügbarkeit
kommen. Dieses Konzept wird bei NoSQL zugunsten der besseren Skalierbarkeit und auch Verfügbarkeit
aufgeweicht."\cite[S. 18]{NoSQLClemensGull}
\end{quote}

Um die bessere Verfügbarkeit bei dokumentenorientierten DBMS zu garantieren, wird bewusst auf die Normalisierung
\footnote{Frank Geisler zur Normalisierung: "Die Normalisierung reduziert die in der Datenbank vorhandenen Datenredundanzen und hilft Ihnen so dabei, die \dots Anomalien zu vermeiden."\cite[S. 177]{DatenbankenFrankGeisler}}
verzichtet. Durch die dadurch entstehende Redundanz in der Datenbank fällt es schwer,
die Konsistenz der Daten zu garantieren. Außerdem ist der Schreibvorgang bei dokumentenorientierten
DBMS wie MongoDB nur auf Dokumentenebene atomar.\cite{MongoDBAtomaritaet} Da die Konsistenz und Atomarität für
Benutzerverwaltungsdaten von erheblicher Relevanz ist\footnote{Speziell bei sensiblen Transaktionen wie Zahlungsabläufen.}, 
entscheidet sich die Arbeit im Fall des Resource Management Services für die Verwendung eines RDBMS.
Um das Performanzdefizit im Vergleich zu dokumentbasierenden DBMS auszugleichen,
werden Indices verwendet. Ein Datenbankindex ist ein Kompromiss, bei dem
man Speicherplatz gegen Zugriffsgeschwindigkeit tauscht.\cite{YoutubePostgresIndexing}
Mithilfe einer aktuell gehaltenen Indexstruktur\footnote{Beispielsweise Binärbäume, Hashtabellen und sortierte Abfolgen}
wird die Komplexität eines Spaltenzugriffs dezimiert.

\subsection{Überwachung und Wartung}
\label{subsec:ueberwachungundwartung}
Essentiell für die Überwachung der Services ist eine solide Protokollierung.
Die Arbeit beschränkt sich hier auf drei Protokollebenen; Error, Info und Debug.
Error ist die tiefste Ebene. Hier werden die während der Laufzeit des Programms
stattgefundenen Fehler protokolliert. Auf der Info-Protokollebene sieht man
aufschlussreiche Informationen über den Service. Hier wird beispielsweise
der HTTP-Verkehr aufgezeichnet. Die Error-Protokollebene ist in der Info-Protokollebene
enthalten. In der Debug-Ebene erscheinen die beiden zuvor erwähnten Ebenen. Diese
werden mit Informationen angereichert, die nur für das Debugging relevant sind.
Ein Logeintrag besteht aus einem Zeitstempel, der Protokollebene sowie der Protokollnachricht.
Kundenspezifische Informationen werden nicht offengelegt. Verweise auf Nutzer finden nur
mithilfe der Identifikationsnummer des Datenbankeintrags statt. Die Info-Protokollebene
wird über den \code{stdout} gelenkt. Die Error-Protokollebene wird des Weiteren in einer
Logdatei persistiert. Für das Debugging muss die Debug-Protokollebene aktiviert werden.
Als Logging-Bibliothek für die Resource Management API verwendet die Arbeit den Winston
Logger. Der HTTP-Verkehr wird mithilfe des Koa-Loggers\footnote{Koa ist ein HTTP Middleware Framework für Node.js} protokolliert und an den Winston
Logger weitergereicht. Datenbankverbinder sowie Logger werden mithilfe des Singleton-Pattern
implementiert, um das ständige herunterreichen von Instanzen zu vermeiden. Der komfortabelste
Weg in JavaScript ein Singleton zu erstellen, ist, indem man ein Objekt exportiert. JavaScript-Objekte
werden beim initialen Ladevorgang in den Cache gesetzt. JavaScript sorgt dafür,
dass bei allen weiteren Zugriffen auf das Objekt, die selbe Instanz zurückgegeben wird.\cite{NodeJsCaching}

Der Reverse Proxy Traefik stellt eine Übersicht über die im HTTP-Verkehr gesendeten 
HTTP-Statuscodes. Die einzelnen Services besitzen Health-Checks, die den aktuellen
Gesundheitsstatus der Serviceinstanz widerspiegeln. Um die Wartung der Services zu erleichtern,
werden bei jedem Pipeline-Durchlauf unbenutzte Container, Netzwerke und Images
entfernt sowie der Cache entleert.

\section{Projektaufbau}
\label{:sec:projektaufbau}

\subsection{Monorepo und Git-Submodules}
\label{subsec:monorepoundsubmodules}
In diesem Abschnitt geht es um die Aufteilung des
Programmcodes in Repositories. Für die Versionsverwaltung
wird Git verwendet. Die Arbeit entscheidet sich dazu,
die einzelnen Microservices in einer Monorepo unterzubringen.
Unter Monorepo versteht man die Strategie, den Programmcode
aus mehreren Anwendungen, Services, Bibliotheken und Frameworks
in einer Repository unterzubringen.\cite{MonorepoTrunkBasedDevelopment}
Diese Strategie verringert nicht nur den Aufwand
des Aufsetzens mehrerer Repositories, sondern gibt den
Entwicklern auch einen besseren Überblick über die einzelnen
Projekte. Gerade im Fall von Microservices hat dieser
Ansatz den Vorteil, dass man Änderungen in mehreren Services,
die miteinander verkoppelt sind, in einem Commit in die
Versionsverwaltung einchecken kann. Dies kann verbindlich sein,
wenn Tests der Pipeline auf bestimmte Versionen der unterschiedlichen
Services angewiesen sind. Verändert man beispielsweise
eine API eines Services, kann man gleichzeitig das Frontend
anpassen. Die Diagramme eines Dashboards sollen allerdings
auch extern entwickelbar sein. Um dies zu ermöglichen, verwendet
die Arbeit Git-Submodules.\cite{GitsubmodulesGitSCM} Mithilfe dieser Submodules
kann man andere Repositories in ein Repository integrieren. Somit können externe Entwickler das Repository des
Submodules gabeln\footnote{Unter "gabeln" oder auch "forken" versteht man
das Erstellen einer eigenen Kopie eines Repositories, mit der unabhängig von
der Versionierung der gegabelten Repository entwickelt werden kann.},
um in ihrem eigenen Repository Diagramme für die Anwendung zu entwickeln.



\section{Pipeline}
\label{sec:pipeline}
Um eine stetige Softwareauslieferung zu ermöglichen, benötigt es einen automatisierten
Prozess. Dieser Prozess muss sich um das Bauen, Testen und Ausliefern der einzelnen
Container kümmern. In Abbildung \ref{figure:uebersichtueberdenstetigenauslieferungsprozess}
sieht man eine Übersicht über den Auslieferungsprozess. An der X-Achse entlang sieht
man die unterschiedlichen Umgebungen. Als erstes ist dort die Pipeline, ein in einem
Gitlab Runner ausgelagerter Prozess, der ein vordefiniertes Script durchläuft.
Damit der Gitlab Runner weiß, was er zu tun hat, werden in der \code{.gitlab-ci.yml}
Datei einzelne Abschnitte und deren Aufgaben mithilfe der vereinfachten
Auszeichnungssprache YAML definiert. Für jeden Abschnitt kann ein eigenes Docker-Image
gewählt werden. Mehr dazu findet man in \Cref{subsec:stages}.

\begin{figure}
    \begin{center}
    \includegraphics[scale=0.2]{img/abbildungen/Pipeline}
    \end{center}
    \caption{Übersicht über den stetigen Auslieferungsprozess}
    \label{figure:uebersichtueberdenstetigenauslieferungsprozess}
\end{figure}

An zweiter Stelle
befindet sich die Docker Registry. Eine Docker Registry ist für die Umgebung
einer Softwareanwendung das, was eine Git Repository für den Quellcode ist.
Eine Docker Registry ist also eine Art Versionsverwaltung für Docker-Images.
Hat man die Images einmal getestet, kann man davon ausgehen, dass es sich
in ausgeliefertem Zustand genauso verhalten wird, wie dies beim Testen der Fall war.

An dritter Stelle kommt die die Umgebung für die Qualitätssicherung.\footnote{Auf English Quality Assurance, abgekürzt QA.}
Hier wird die Software nach ihrer Qualität getestet. Erst wenn sie eine
zuvor definierte Qualitätsmerkmale aufweist, darf diese in die Produktionsumgebung
ausgeliefert werden.

An letzter Stelle kommt die Produktionsumgebung. Hier werden die laufenden Container durch
die neuen Container, welche alle Tests erfolgreich absolviert haben, ersetzt. Die Produktionsumgebung
unterscheidet sich von der QA-Umgebung daran, dass hier die einzelnen Container je nach Verarbeitungsmenge
skaliert werden.

Als nächstes werden in \Cref{subsec:stages} die einzelnen Stages, also die Abschnitte der Pipeline
beschrieben. Daraufhin geht die Arbeit in \Cref{subsec:dockercomposesetup} auf den Aufbau von Docker-Compose ein.
In \Cref{subsec:umgebungsvariablen} wird beschrieben, wie die Arbeit mit den Umgebungsvariablen umgeht.
Zuallerletzt wird in \Cref{subsec:healthcheck} das Konzept der Health-Checks beschrieben.

\subsection{Stages}
\label{subsec:stages}
In Abbildung \ref{figure:uebersichtueberdenstetigenauslieferungsprozess} des vorherigen Abschnitts
sieht man auf der Y-Achse die Stages, die die Pipeline bei dem Auslieferungsprozess durchläuft.
Genauso wie das System der eigentlichen Anwendung besteht auch die Pipeline aus unterschiedlichen
Docker-Containern. Somit ist jeder der Abschnitte unabhängig von dem Gesamtgeschehen der Pipeline.
Öfter auftretende Funktionalitäten können in der Gitlab-CI-Pipeline in eigene Scripts ausgelagert
werden. So verhindert man redundanten Quellcode in der Pipeline. Ein Beispiel hierfür
ist Quellcode \ref{lst:wiederverwendbarescriptsdergitlabci}. Das erste ausgelagerte
Script ermöglicht das verwenden von SSH innerhalb der Pipeline. Somit kann sich
die pipeline mit dem Server verbinden, um die Docker-Compose Datei auf den Server
zu kopieren und diese auszuführen. Das zweite Script ist dafür zuständig,
sich in die Docker Registry einzuloggen. Dies ist nötig, um Docker-Container
in die Registry zu speichern und diese später vom Server aus wieder abzufragen.

\begin{listing}
    \inputminted{yaml}{snippets/yml/reusable_scripts.yml}
    \caption{Wiederverwendbare Scripts der Gitlab-CI}
    \label{lst:wiederverwendbarescriptsdergitlabci}
\end{listing}

Für die Build- und Deploy-Stage wird ein von Docker vorgefertigtes Git-Image verwendet.
Die Test-Stage verwendet ein vorgefertigtes Node.js Image mit der Version 12. Dies ist notwendig,
um die Integrationstests auszuführen, welche in JavaScript geschrieben sind.

\subsection{Health-Checks}
\label{subsec:healthcheck}
In Microservice-Infrastrukturen sind die einzelnen Services eng miteinander verknüpft
und voneinander abhängig. Um die Verfügbarkeit sicherzustellen, werden auf
Serviceebene Healthcheckendpunkte bereitgestellt. Diese spiegeln den aktuellen
Gesundheitszustand des jeweiligen Dienstes wieder. Ein Healthcheckendpunkt
ist speziell dann nützlich, wenn ein Dienst oder ein Verfahren von einem anderen
abhängig ist. So ist beispielsweise die Backendapi von der Datenbank
abhängig. Andererseits ist der Frontenddienst sowie das Apitestverfahren
in der Buildpipeline von der Backendapi abhängig. 

\begin{listing}
    \label{lst:healthcheck}
    \inputminted{sh}{snippets/sh/healthcheck.sh}
    \caption{Healthcheckbeispiel in der Gitlab CI}
\end{listing}

\section{Frontendanwendung}
\label{sec:frontendanwendung}

\subsection{Android Anwendung}
\label{subec:androidanwendung}

\subsection{Zwischenspeicherstrategie im Service Worker}
\label{subsec:zwischenspeicherstrategieimserviceworker}
Ein Service Worker ist ein in JavaScript geschriebener,
eventbasierter Proxy, welcher in einem separaten Thread im Browser
läuft und an eine spezifische Origin gekoppelt ist. Er hat
die Möglichkeit Anfragen zwischen der im Browser ausgeführten
Anwendung und dem Server abzufangen und zu verarbeiten.

Cache Only
Network Only
Cache First
Network First
Cache then Network
Stale while Revalidate
Generic Fallback

\begin{description}
    \item[Für den Zwischenspeicher des Browsers relevante Untergliederung]~\par
    \begin{itemize}
       \item Statische Dateien mit Hash
       \begin{itemize}
            \item main.dd5a1ad0.chunk.css
            \item main.46e36a4b.chunk.js
            \item 2.dc039c03.chunk.js
       \end{itemize}
       \item Statische Dateien ohne Hash
       \begin{itemize}
            \item index.html
            \item manifest.json
            \item favicon.ico
       \end{itemize}
       \item JSON Ressourcen von AJAX Anfragen
    \end{itemize}
\end{description}

Daraus folgende Zwischenspeicherstrategien:

\begin{description}
    \item[Zwischenspeicherstrategie]~\par
    \begin{enumerate}
       \item Statische Dateien mit Hash
       \begin{enumerate}
          \item Zuerst aus dem Zwischenspeicher anfragen
          \item Falls nicht vorhanden, aus dem Netzwerk laden und in den Zwischenspeicher ablegen
       \end{enumerate}
       \item Statische Dateien ohne Hash
       \begin{enumerate}
            \item Zuerst aus dem Netzwerk anfragen und in den Zwischenspeicher ablegen
            \item Falls keine Netzwerkverbindung vorhanden, aus dem Zwischenspeicher laden
        \end{enumerate}
        \item JSON Ressourcen von AJAX Anfragen
        \begin{enumerate}
             \item Zuerst aus dem Netzwerk anfragen
             \item Falls nicht vorhanden, Offline-Rückfallseite anzeigen 
         \end{enumerate}
    \end{enumerate}
 \end{description}

\section{Plugins}
\label{sec:plugins}

\section{Resource Management Service}
\label{sec:resourcemanagementservice}

\section{Data Delivery Service}
\label{sec:datadeliveryservice}

\begin{figure}
    \label{figure:informationsaustauschdashboard}
    \begin{center}
    \includegraphics[scale=0.2]{img/abbildungen/InformationsaustauschDashboard}
    \end{center}
    \caption{Ablauf des Informationsaustausches eines Dashboards}
\end{figure}

